{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCigm_4gsRqP"
   },
   "source": [
    "<img src=\"https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png\" srcset=\"https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_130 130w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_260 260w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_390 390w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_520 520w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_650 650w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_780 780w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_910 910w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_1040 1040w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_1170 1170w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_1290 1290w\" sizes=\"100vw\" width=\"1290\">\n",
    "\n",
    "# Simple Chat with Message History\n",
    "\n",
    "This notebook demonstrates how to create a simple chat interface with message history using LangChain. The chat will:\n",
    "\n",
    "1. Maintain a history of all messages\n",
    "2. Allow adding new messages\n",
    "3. Use the history as context for responses\n",
    "4. Store both user and AI messages in the conversation\n",
    "\n",
    "Key concepts covered:\n",
    "- Message history management\n",
    "- Context-aware responses\n",
    "- LangChain chat components\n",
    "- LLM integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxta2VlVsRqR"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18295,
     "status": "ok",
     "timestamp": 1732539072005,
     "user": {
      "displayName": "Sebastian Gingter",
      "userId": "03749202599653192620"
     },
     "user_tz": -60
    },
    "id": "_KVwuRvDsRqS",
    "outputId": "7710e43d-349c-48d5-838b-e89c65f38b85"
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain==0.3.7 langchain-openai==0.2.5 langchain-community==0.3.5 langfuse==2.53.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7MQuotFsRqS"
   },
   "source": [
    "### Read environment variables from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1774,
     "status": "ok",
     "timestamp": 1732539073776,
     "user": {
      "displayName": "Sebastian Gingter",
      "userId": "03749202599653192620"
     },
     "user_tz": -60
    },
    "id": "GrPKAJP4sRqS"
   },
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare LangFuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "langfuse_handler = CallbackHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QCXx9dpsRqS"
   },
   "source": [
    "## Available LLMs\n",
    "\n",
    "Let's use OpenAI for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1732539076624,
     "user": {
      "displayName": "Sebastian Gingter",
      "userId": "03749202599653192620"
     },
     "user_tz": -60
    },
    "id": "F_Gvp7RqsRqT"
   },
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "TEMPERATURE = 0\n",
    "MAX_TOKENS = 1500\n",
    "\n",
    "MODEL_NAME = \"gpt-4o\"\n",
    "BASE_URL = \"https://api.openai.com/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWInVnbtsRqT"
   },
   "source": [
    "## Initialize Chat Components\n",
    "\n",
    "Now we'll set up our chat model and message history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4368,
     "status": "ok",
     "timestamp": 1732539090261,
     "user": {
      "displayName": "Sebastian Gingter",
      "userId": "03749202599653192620"
     },
     "user_tz": -60
    },
    "id": "VfPPaalGsRqT"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Initialize the chat model\n",
    "chat = ChatOpenAI(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    base_url=BASE_URL\n",
    ")\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create the basic chain\n",
    "chain = prompt | chat\n",
    "\n",
    "# Initialize message history\n",
    "message_history = ChatMessageHistory()\n",
    "\n",
    "# Create a chain with message history\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfb8OgQSsRqU"
   },
   "source": [
    "## Chat Interface\n",
    "\n",
    "Let's create a simple function to handle chat interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1732539353936,
     "user": {
      "displayName": "Sebastian Gingter",
      "userId": "03749202599653192620"
     },
     "user_tz": -60
    },
    "id": "UEbFiYGysRqU"
   },
   "outputs": [],
   "source": [
    "def chat_with_history(user_input: str):\n",
    "    \"\"\"Send a message to the chat and get a response\"\"\"\n",
    "    response = chain_with_history.invoke(\n",
    "        {\"input\": user_input},\n",
    "        {\"configurable\": {\"session_id\": \"demo\"}, \"callbacks\":[langfuse_handler]})\n",
    "\n",
    "    print(f\"Assistant: {response.content}\")\n",
    "    print(\"\\nCurrent message history:\")\n",
    "    for msg in message_history.messages:\n",
    "        print(f\"{msg.type.capitalize()}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCpBgKi2sRqU"
   },
   "source": [
    "## Try it out!\n",
    "\n",
    "Let's test our chat with some example interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1924,
     "status": "ok",
     "timestamp": 1732539364187,
     "user": {
      "displayName": "Sebastian Gingter",
      "userId": "03749202599653192620"
     },
     "user_tz": -60
    },
    "id": "CcG3gQtWsRqU",
    "outputId": "88358364-8e50-4f5d-d501-9c335bde8e2c"
   },
   "outputs": [],
   "source": [
    "chat_with_history(\"Hi! My name is Alice.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 925,
     "status": "ok",
     "timestamp": 1732539377876,
     "user": {
      "displayName": "Sebastian Gingter",
      "userId": "03749202599653192620"
     },
     "user_tz": -60
    },
    "id": "QyeR8PrgsRqU",
    "outputId": "f7239103-5c66-4b70-a5d8-140108ca3a65"
   },
   "outputs": [],
   "source": [
    "chat_with_history(\"What's my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kim_0o7XsRqU"
   },
   "source": [
    "## Your Turn!\n",
    "\n",
    "Type your message in the cell below and run it to chat with the AI. The assistant will remember the context of your conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1083,
     "status": "ok",
     "timestamp": 1732539388041,
     "user": {
      "displayName": "Sebastian Gingter",
      "userId": "03749202599653192620"
     },
     "user_tz": -60
    },
    "id": "Rfvc9kP5sRqV",
    "outputId": "fca5662b-7af9-4b0e-e8b0-bf08da21e227"
   },
   "outputs": [],
   "source": [
    "chat_with_history(\"Your message here\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
